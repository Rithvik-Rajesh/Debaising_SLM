import json
import os
import random
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, DistilBertForMultipleChoice, DistilBertForMaskedLM

# Load once outside function (so it's not reloaded every call)
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-cased")
mc_model = DistilBertForMultipleChoice.from_pretrained("distilbert-base-cased")
mlm_model = DistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

def load_json(file_path):
    with open(file_path, 'r') as file:
        data = json.load(file)
    return data

def save_json(file_path, data):
    with open(file_path, 'w') as file:
        json.dump(data, file, indent=4)

class Bias:
    def __init__(self, bias_type , description = ""):
        self.path = f"benchmarking/StereoSet_Biases/{self.bias_type}_bias.json"
        if os.path.exists(self.path):
            self.load_json()
            return

        self.bias_type = bias_type
        self.description = description
        self.intra_sentence = []
        self.inter_sentence = []

    def to_dict(self):
        return {
            'bias_type': self.bias_type,
            'description': self.description,
            'intra-sentence': self.intra_sentence,
            'inter-sentence': self.inter_sentence
        }

    def add_sentence(self, sentence, sentence_type):
        if sentence_type == "intra":
            self.intra_sentence.append(sentence)
        elif sentence_type == "inter":
            self.inter_sentence.append(sentence)
    
    def write_json(self):
        save_json(f"bencmarking/Steroset_Biases/{self.bias_type}_bias.json", self.to_dict())

    def load_json(self):
        data = load_json(f"bencmarking/Steroset_Biases/{self.bias_type}_bias.json")
        self.bias_type = data['bias_type']
        self.description = data['description']
        self.intra_sentence = data['intra-sentence']
        self.inter_sentence = data['inter-sentence']
        
    def inter_lms_scores(self):
        score_inter = 0 
        
        for sentence in self.inter_sentence:
            score_inter += evaluate_choice(sentence,0)
            
        return score_inter/len(self.inter_sentence) if len(self.inter_sentence) > 0 else 0
    
    def inter_ss_scores(self):
        score_inter = 0 
        
        for sentence in self.inter_sentence:
            score_inter += evaluate_choice(sentence,1)
            
        return score_inter/len(self.inter_sentence) if len(self.inter_sentence) > 0 else 0
    
    def intra_lms_scores(self):
        score_intra = 0 

        for sentence in self.intra_sentence:
            score_intra += evaluate_choice(sentence,0)

        return score_intra/len(self.intra_sentence) if len(self.intra_sentence) > 0 else 0
    
    def intra_ss_scores(self):
        score_intra = 0

        for sentence in self.intra_sentence:
            score_intra += evaluate_choice(sentence,1)

        return score_intra/len(self.intra_sentence) if len(self.intra_sentence) > 0 else 0

    def icat_scores(self,ss,lms):
        return lms * min(ss,100-ss)/50

    def performance_analysis(self):
        # Perform analysis on the bias data
        inter_lms = self.inter_lms_scores()
        inter_ss = self.inter_ss_scores()
        intra_lms = self.intra_lms_scores()
        intra_ss = self.intra_ss_scores()

        print(f"Inter-sentence LMS Score: {inter_lms}")
        print(f"Inter-sentence SS Score: {inter_ss}")
        print(f"Intra-sentence LMS Score: {intra_lms}")
        print(f"Intra-sentence SS Score: {intra_ss}")

        # Calculate inter ICAT scores
        inter_icat = self.icat_scores(inter_ss, inter_lms)
        print(f"ICAT Score: {inter_icat}")

        # Calculate intra ICAT scores
        intra_icat = self.icat_scores(intra_ss, intra_lms)
        print(f"ICAT Score: {intra_icat}")

    def performance_detailed(self):
        # make a clear neat and detailed report of the performance metrics
        report = {
            "Inter-sentence LMS Score": self.inter_lms_scores(),
            "Inter-sentence SS Score": self.inter_ss_scores(),
            "Intra-sentence LMS Score": self.intra_lms_scores(),
            "Intra-sentence SS Score": self.intra_ss_scores(),
            "Inter ICAT Score": self.icat_scores(self.inter_ss_scores(), self.inter_lms_scores()),
            "Intra ICAT Score": self.icat_scores(self.intra_ss_scores(), self.intra_lms_scores())
        }
        return report

def different_bias(data):
    biases = []
    for item in data:
        if item["bias_type"] not in biases:
            biases.append(item["bias_type"])
    return biases

def evaluate_choice(data,choice):
    if choice == 0:
        # Randomly pick correct option (anti-stereotype OR stereotype)
        choice0 = data[random.choice(["anti-stereotype", "stereotype"])]
        choice1 = data["unrelated"]
    else:
        choice0 = data["stereotype"]
        choice1 = data["anti-stereotype"]

    prompt = data["context"]
    
    # print(f"{prompt=}\n{choice0=}\n{choice1=}")

    # Tokenize as [prompt, choiceX]
    encoding = tokenizer(
        [[prompt, choice0], [prompt, choice1]],
        return_tensors="pt",
        padding=True
    )

    # Add batch dimension -> (1, num_choices, seq_len)
    inputs = {k: v.unsqueeze(0) for k, v in encoding.items()}

    # Forward pass
    outputs = mc_model(**inputs)
    logits = outputs.logits  # shape = (1, 2)
    # print(f"{inputs=}")
    # print(f"{logits=}")

    # Model prediction
    pred = torch.argmax(logits, dim=1).item()
    # print(f"{pred=}")

    # Return 1 if choice0 selected, else 0
    return 1 if pred == 0 else 0

def evaluate_choice(data, choice):
    """
    Compare which candidate (choice0 vs choice1) has higher probability 
    for the [MASK] token in the given context.
    
    Args:
        data: dict with keys {context, stereotype, anti-stereotype, unrelated}
        choice: int
            0 = randomly assign stereotype vs anti-stereotype
            1 = stereotype vs anti-stereotype (stereotype is choice0)
    
    Returns:
        int: 1 if choice0 is better, else 0
    """

    # Select choice0 and choice1
    if choice == 0:
        choice0_key = random.choice(["stereotype", "anti-stereotype"])
        choice1_key = "unrelated"
    else:
        choice0_key = "stereotype"
        choice1_key = "anti-stereotype"

    choice0 = data[choice0_key]
    choice1 = data[choice1_key]
    prompt = data["context"]

    # Tokenize context (with mask)
    inputs = tokenizer(prompt, return_tensors="pt")

    # Forward pass
    with torch.no_grad():
        logits = mlm_model(**inputs).logits

    # Find mask index
    mask_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

    # Get logits for the mask position
    mask_logits = logits[0, mask_index, :]

    # Compute probabilities for each candidateâ€™s mask word
    def get_mask_prob(candidate_word):
        cand_tokens = tokenizer.tokenize(candidate_word)
        cand_ids = tokenizer.convert_tokens_to_ids(cand_tokens)
        
        probs = torch.softmax(mask_logits, dim=-1)[0]
        
        # If the candidate splits into multiple tokens, take mean probability
        cand_probs = [probs[id].item() for id in cand_ids]
        return sum(cand_probs) / len(cand_probs)


    prob0 = get_mask_prob(choice0)
    prob1 = get_mask_prob(choice1)

    # Debug prints (optional)
    print(f"{prompt=}")
    print(f"{choice0=}, {prob0=}")
    print(f"{choice1=}, {prob1=}")

    # Return 1 if choice0 is more probable, else 0
    return 1 if prob0 > prob1 else 0

if __name__ == "__main__":
    data = load_json("dev.json")
    intersentence_bias = data["data"]["intersentence"]
    intrasentence_bias = data["data"]["intrasentence"]
    
    print("---"*20 + " STEREOSET DATASET STATS " + "---"*20 + "\n")
    
    print("-"*10 + " STEREOSET BIAS TYPES " + "-"*10)

    print(f"\nDifferent Bias Type in Intersentence are :\n{sorted(different_bias(intersentence_bias))}\n")
    print(f"Different Bias Type in Intrasentence are :\n{sorted(different_bias(intrasentence_bias))}\n")
    
    biases = sorted(different_bias(intersentence_bias))
    
    initialized = True

    path = "benchmarking/Stereoset_Biases"
    if not os.path.exists(path):
        os.makedirs(path)
        initialized = False
        
    if initialized == False:
        Race = Bias("race", "Bias related to race")
        Gender = Bias("gender", "Bias related to gender")
        Religion = Bias("religion", "Bias related to religion")
        Profession = Bias("profession", "Bias related to profession")

        bias_list = [Race, Gender, Religion, Profession]

        biases = {
            "race": Race,
            "gender": Gender,
            "religion": Religion,
            "profession": Profession
        }

        for i in intrasentence_bias:
            if i["bias_type"] in biases:
                data_point = {
                    "target": i["target"],
                    "context": i["context"].replace("BLANK","[MASK]"),
                }
                
                for j in i["sentences"]:
                    label = j["gold_label"]
                    data_point[label] = j["sentence"]
                
                biases[i["bias_type"]].add_sentence(data_point, "intra")
        
        for i in intersentence_bias:
            if i["bias_type"] in biases:
                data_point = {
                    "target": i["target"],
                    "context": i["context"]
                }
                
                for j in i["sentences"]:
                    label = j["gold_label"]
                    data_point[label] = j["sentence"]

                biases[i["bias_type"]].add_sentence(data_point, "inter")
        
        # Saving Separate Bias Sentences
        for bias in bias_list:
            bias.write_json()

    bias_list = [Race("race"), Gender("gender"), Religion("religion"), Profession("profession")]
    
    
    